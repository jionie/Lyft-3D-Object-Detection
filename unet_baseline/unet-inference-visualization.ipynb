{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "import gc\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces\n",
    "# even more threads which would lead to a lot of context switching, slowing things down a lot.\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR, _LRScheduler\n",
    "import torch.nn.utils.weight_norm as weightNorm\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "\n",
    "from models.model import *\n",
    "import torchvision.models as models\n",
    "\n",
    "from utils.transform import *\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from apex import amp\n",
    "from ranger import *\n",
    "\n",
    "import albumentations\n",
    "from albumentations import torch as AT\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.eval.detection.mAP_evaluation import Box3D, recall_precision\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n",
    "from lyft_dataset_sdk.utils.map_mask import MapMask\n",
    "from pathlib import Path\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset,LyftDatasetExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "17 attribute,\n",
      "8 sensor,\n",
      "168 calibrated_sensor,\n",
      "219744 ego_pose,\n",
      "218 log,\n",
      "218 scene,\n",
      "27468 sample,\n",
      "219744 sample_data,\n",
      "1 map,\n",
      "Done loading in 1.7 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.5 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "############################################################################## Some hyperparameters we'll need to define for the system\n",
    "voxel_size = (0.4, 0.4, 1.5)\n",
    "z_offset = -2.0\n",
    "bev_shape = (336, 336, 3)\n",
    "\n",
    "# We scale down each box so they are more separated when projected into our coarse voxel space.\n",
    "box_scale = 0.8\n",
    "\n",
    "# If you try to use original LyftDataset Class, you will get missing table error\n",
    "class LyftTestDataset(LyftDataset):\n",
    "    \"\"\"Database class for Lyft Dataset to help query and retrieve information from the database.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, json_path: str, verbose: bool = True, map_resolution: float = 0.1):\n",
    "        \"\"\"Loads database and creates reverse indexes and shortcuts.\n",
    "        Args:\n",
    "            data_path: Path to the tables and data.\n",
    "            json_path: Path to the folder with json files\n",
    "            verbose: Whether to print status messages during load.\n",
    "            map_resolution: Resolution of maps (meters).\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = Path(data_path).expanduser().absolute()\n",
    "        self.json_path = Path(json_path)\n",
    "\n",
    "        self.table_names = [\n",
    "            \"category\",\n",
    "            \"attribute\",\n",
    "            \"sensor\",\n",
    "            \"calibrated_sensor\",\n",
    "            \"ego_pose\",\n",
    "            \"log\",\n",
    "            \"scene\",\n",
    "            \"sample\",\n",
    "            \"sample_data\",\n",
    "            \"map\",\n",
    "        ]\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Explicitly assign tables to help the IDE determine valid class members.\n",
    "        self.category = self.__load_table__(\"category\")\n",
    "        self.attribute = self.__load_table__(\"attribute\")\n",
    "        \n",
    "        \n",
    "        self.sensor = self.__load_table__(\"sensor\")\n",
    "        self.calibrated_sensor = self.__load_table__(\"calibrated_sensor\")\n",
    "        self.ego_pose = self.__load_table__(\"ego_pose\")\n",
    "        self.log = self.__load_table__(\"log\")\n",
    "        self.scene = self.__load_table__(\"scene\")\n",
    "        self.sample = self.__load_table__(\"sample\")\n",
    "        self.sample_data = self.__load_table__(\"sample_data\")\n",
    "        \n",
    "        self.map = self.__load_table__(\"map\")\n",
    "\n",
    "        if verbose:\n",
    "            for table in self.table_names:\n",
    "                print(\"{} {},\".format(len(getattr(self, table)), table))\n",
    "            print(\"Done loading in {:.1f} seconds.\\n======\".format(time.time() - start_time))\n",
    "\n",
    "        # Initialize LyftDatasetExplorer class\n",
    "        self.explorer = LyftDatasetExplorer(self)\n",
    "        # Make reverse indexes for common lookups.\n",
    "        self.__make_reverse_index__(verbose)\n",
    "        \n",
    "    def __make_reverse_index__(self, verbose: bool) -> None:\n",
    "        \"\"\"De-normalizes database to create reverse indices for common cases.\n",
    "        Args:\n",
    "            verbose: Whether to print outputs.\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"Reverse indexing ...\")\n",
    "\n",
    "        # Store the mapping from token to table index for each table.\n",
    "        self._token2ind = dict()\n",
    "        for table in self.table_names:\n",
    "            self._token2ind[table] = dict()\n",
    "\n",
    "            for ind, member in enumerate(getattr(self, table)):\n",
    "                self._token2ind[table][member[\"token\"]] = ind\n",
    "\n",
    "        # Decorate (adds short-cut) sample_data with sensor information.\n",
    "        for record in self.sample_data:\n",
    "            cs_record = self.get(\"calibrated_sensor\", record[\"calibrated_sensor_token\"])\n",
    "            sensor_record = self.get(\"sensor\", cs_record[\"sensor_token\"])\n",
    "            record[\"sensor_modality\"] = sensor_record[\"modality\"]\n",
    "            record[\"channel\"] = sensor_record[\"channel\"]\n",
    "\n",
    "        # Reverse-index samples with sample_data and annotations.\n",
    "        for record in self.sample:\n",
    "            record[\"data\"] = {}\n",
    "            record[\"anns\"] = []\n",
    "\n",
    "        for record in self.sample_data:\n",
    "            if record[\"is_key_frame\"]:\n",
    "                sample_record = self.get(\"sample\", record[\"sample_token\"])\n",
    "                sample_record[\"data\"][record[\"channel\"]] = record[\"token\"]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Done reverse indexing in {:.1f} seconds.\\n======\".format(time.time() - start_time))\n",
    "\n",
    "level5data = LyftTestDataset(data_path='.', json_path='/media/jionie/my_disk/Kaggle/Lyft/input/3d-object-detection-for-autonomous-vehicles/test_root/data', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 336\n",
    "\n",
    "def transform_train(image, mask):\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.RandomRotate90(p=1)(image=image)['image']\n",
    "        mask = albumentations.RandomRotate90(p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.Transpose(p=1)(image=image)['image']\n",
    "        mask = albumentations.Transpose(p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.VerticalFlip(p=1)(image=image)['image']\n",
    "        mask = albumentations.VerticalFlip(p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.HorizontalFlip(p=1)(image=image)['image']\n",
    "        mask = albumentations.HorizontalFlip(p=1)(image=mask)['image']\n",
    "\n",
    "    # if random.random() < 0.5:\n",
    "    #     image = albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.15, rotate_limit=45, p=1)(image=image)['image']\n",
    "    #     mask = albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.15, rotate_limit=45, p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.RandomBrightness(0.1)(image=image)['image']\n",
    "        image = albumentations.RandomContrast(0.1)(image=image)['image']\n",
    "        image = albumentations.Blur(blur_limit=3)(image=image)['image']\n",
    "\n",
    "    # if random.random() < 0.5:\n",
    "    #     image = albumentations.Cutout(num_holes=1, max_h_size=32, max_w_size=32, p=1)(image)\n",
    "    #     mask = albumentations.Cutout(num_holes=1, max_h_size=32, max_w_size=32, p=1)(mask)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def transform_valid(image, mask):\n",
    "    # if random.random() < 0.5:\n",
    "    #     image = albumentations.RandomRotate90(p=1)(image=image)['image']\n",
    "    #     mask = albumentations.RandomRotate90(p=1)(image=mask)['image']\n",
    "\n",
    "    # if random.random() < 0.5:\n",
    "    #     image = albumentations.Transpose(p=1)(image=image)['image']\n",
    "    #     mask = albumentations.Transpose(p=1)(image=mask)['image']\n",
    "\n",
    "    # if random.random() < 0.5:\n",
    "    #     image = albumentations.VerticalFlip(p=1)(image=image)['image']\n",
    "    #     mask = albumentations.VerticalFlip(p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.HorizontalFlip(p=1)(image=image)['image']\n",
    "        mask = albumentations.HorizontalFlip(p=1)(image=mask)['image']\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def transform_test(image):\n",
    "    \n",
    "    image_hard = image.copy()\n",
    "    image_simple = image.copy()\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image_hard = albumentations.RandomBrightness(0.1)(image=image_hard)['image']\n",
    "        image_hard = albumentations.RandomContrast(0.1)(image=image_hard)['image']\n",
    "        image_hard = albumentations.Blur(blur_limit=3)(image=image_hard)['image']\n",
    "\n",
    "    return image_simple, image_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get sample_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2409e326a7af4d0d8194c1013d3b563f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=218), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of tokens= 27468\n"
     ]
    }
   ],
   "source": [
    "records = [(level5data.get('sample', record['first_sample_token'])['timestamp'], record) for record in level5data.scene]\n",
    "\n",
    "entries = []\n",
    "\n",
    "for start_time, record in sorted(records):\n",
    "    start_time = level5data.get('sample', record['first_sample_token'])['timestamp'] / 1000000\n",
    "\n",
    "    token = record['token']\n",
    "    name = record['name']\n",
    "    date = datetime.utcfromtimestamp(start_time)\n",
    "    host = \"-\".join(record['name'].split(\"-\")[:2])\n",
    "    first_sample_token = record[\"first_sample_token\"]\n",
    "\n",
    "    entries.append((host, name, date, token, first_sample_token))\n",
    "            \n",
    "df = pd.DataFrame(entries, columns=[\"host\", \"scene_name\", \"date\", \"scene_token\", \"first_sample_token\"])\n",
    "\n",
    "# sample_sub = pd.read_csv('../input/3d-object-detection-for-autonomous-vehicles/sample_submission.csv')\n",
    "all_sample_tokens,scene_len = [],[]\n",
    "for sample_token in tqdm_notebook(df.first_sample_token.values):\n",
    "    i = 0\n",
    "    while sample_token:\n",
    "        all_sample_tokens.append(sample_token)\n",
    "        sample = level5data.get(\"sample\", sample_token)\n",
    "        sample_token = sample[\"next\"]\n",
    "        i += 1\n",
    "    scene_len.append(i)\n",
    "#     print(len(all_sample_tokens[-1]))\n",
    "    \n",
    "print('Total number of tokens=',len(all_sample_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEVImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sample_token, test_data_folder=None, img_size=336):\n",
    "        self.sample_token = sample_token\n",
    "        self.test_data_folder = test_data_folder\n",
    "        self.type = type\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_token)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample_token = self.sample_token[idx]\n",
    "        \n",
    "        input_filepath = os.path.join(self.test_data_folder,f\"{sample_token}_input.png\")\n",
    "        map_filepath = os.path.join(self.test_data_folder,f\"{sample_token}_map.png\")\n",
    "        \n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    " \n",
    "        im = np.concatenate((im, map_im), axis=2)\n",
    "\n",
    "        im, _ = transform_test(im) # im_simple, im_hard\n",
    "        \n",
    "        im = torch.from_numpy(im.transpose(2,0,1))\n",
    "\n",
    "        return im, sample_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_height = {'animal':0.51,'bicycle':1.44,'bus':3.44,'car':1.72,'emergency_vehicle':2.39,'motorcycle':1.59,\n",
    "                'other_vehicle':3.23,'pedestrian':1.78,'truck':3.44}\n",
    "\n",
    "class_width = {'animal':0.36,'bicycle':0.63,'bus':2.96,'car':1.93,'emergency_vehicle':2.45,'motorcycle':0.96,\n",
    "                'other_vehicle':2.79,'pedestrian':0.77,'truck':2.84}\n",
    "\n",
    "class_len = {'animal':0.73,'bicycle':1.76,'bus':12.34,'car':4.76,'emergency_vehicle':6.52,'motorcycle':2.35,\n",
    "                'other_vehicle':8.20,'pedestrian':0.81,'truck':10.24}\n",
    "\n",
    "\n",
    "\n",
    "test_data_folder = \"/media/jionie/my_disk/Kaggle/Lyft/input/3d-object-detection-for-autonomous-vehicles/test_root/bev_data_with_map/\"\n",
    "\n",
    "test_input_filepaths = sorted(glob.glob(os.path.join(test_data_folder, \"*_input.png\")))\n",
    "test_target_filepaths = sorted(glob.glob(os.path.join(test_data_folder, \"*_target.png\")))\n",
    "\n",
    "test_dataset = BEVImageDataset(sample_token=all_sample_tokens, test_data_folder=test_data_folder, img_size=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "background_threshold = 40\n",
    "\n",
    "def calc_detection_box(prediction_opened,class_probability):\n",
    "\n",
    "    sample_boxes = []\n",
    "    sample_detection_scores = []\n",
    "    sample_detection_classes = []\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(prediction_opened, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "    \n",
    "    for cnt in contours:\n",
    "        rect = cv2.minAreaRect(cnt)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        \n",
    "        # Let's take the center pixel value as the confidence value\n",
    "        box_center_index = np.int0(np.mean(box, axis=0))\n",
    "        \n",
    "        for class_index in range(len(classes)):\n",
    "            box_center_value = class_probability[class_index+1, box_center_index[1], box_center_index[0]]\n",
    "            \n",
    "            # Let's remove candidates with very low probability\n",
    "            if box_center_value < 0.01:\n",
    "                continue\n",
    "            \n",
    "            box_center_class = classes[class_index]\n",
    "\n",
    "            box_detection_score = box_center_value\n",
    "            sample_detection_classes.append(box_center_class)\n",
    "            sample_detection_scores.append(box_detection_score)\n",
    "            sample_boxes.append(box)\n",
    "            \n",
    "    return np.array(sample_boxes),sample_detection_scores,sample_detection_classes\n",
    "\n",
    "\n",
    "def open_preds(predictions_non_class0):\n",
    "\n",
    "    predictions_opened = np.zeros((predictions_non_class0.shape), dtype=np.uint8)\n",
    "\n",
    "    for i, p in enumerate(tqdm(predictions_non_class0)):\n",
    "        thresholded_p = (p > background_threshold).astype(np.uint8)\n",
    "        predictions_opened[i] = cv2.morphologyEx(thresholded_p, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "    return predictions_opened\n",
    "\n",
    "def create_transformation_matrix_to_voxel_space(shape, voxel_size, offset):\n",
    "    \"\"\"\n",
    "    Constructs a transformation matrix given an output voxel shape such that (0,0,0) ends up in the center.\n",
    "    Voxel_size defines how large every voxel is in world coordinate, (1,1,1) would be the same as Minecraft voxels.\n",
    "    \n",
    "    An offset per axis in world coordinates (metric) can be provided, this is useful for Z (up-down) in lidar points.\n",
    "    \"\"\"\n",
    "    \n",
    "    shape, voxel_size, offset = np.array(shape), np.array(voxel_size), np.array(offset)\n",
    "    \n",
    "    tm = np.eye(4, dtype=np.float32)\n",
    "    translation = shape/2 + offset/voxel_size\n",
    "    \n",
    "    tm = tm * np.array(np.hstack((1/voxel_size, [1])))\n",
    "    tm[:3, 3] = np.transpose(translation)\n",
    "    return tm\n",
    "\n",
    "def transform_points(points, transf_matrix):\n",
    "    \"\"\"\n",
    "    Transform (3,N) or (4,N) points using transformation matrix.\n",
    "    \"\"\"\n",
    "    if points.shape[0] not in [3,4]:\n",
    "        raise Exception(\"Points input should be (3,N) or (4,N) shape, received {}\".format(points.shape))\n",
    "    return transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]\n",
    "\n",
    "\n",
    "def car_to_voxel_coords(points, shape, voxel_size, z_offset=0):\n",
    "    if len(shape) != 3:\n",
    "        raise Exception(\"Voxel volume shape should be 3 dimensions (x,y,z)\")\n",
    "        \n",
    "    if len(points.shape) != 2 or points.shape[0] not in [3, 4]:\n",
    "        raise Exception(\"Input points should be (3,N) or (4,N) in shape, found {}\".format(points.shape))\n",
    "\n",
    "    tm = create_transformation_matrix_to_voxel_space(shape, voxel_size, (0, 0, z_offset))\n",
    "    p = transform_points(points, tm)\n",
    "    return p\n",
    "\n",
    "def create_voxel_pointcloud(points, shape, voxel_size=(0.5,0.5,1), z_offset=0):\n",
    "\n",
    "    points_voxel_coords = car_to_voxel_coords(points.copy(), shape, voxel_size, z_offset)\n",
    "    points_voxel_coords = points_voxel_coords[:3].transpose(1,0)\n",
    "    points_voxel_coords = np.int0(points_voxel_coords)\n",
    "    \n",
    "    bev = np.zeros(shape, dtype=np.float32)\n",
    "    bev_shape = np.array(shape)\n",
    "\n",
    "    within_bounds = (np.all(points_voxel_coords >= 0, axis=1) * np.all(points_voxel_coords < bev_shape, axis=1))\n",
    "    \n",
    "    points_voxel_coords = points_voxel_coords[within_bounds]\n",
    "    coord, count = np.unique(points_voxel_coords, axis=0, return_counts=True)\n",
    "        \n",
    "    # Note X and Y are flipped:\n",
    "    bev[coord[:,1], coord[:,0], coord[:,2]] = count\n",
    "    \n",
    "    return bev\n",
    "\n",
    "def normalize_voxel_intensities(bev, max_intensity=16):\n",
    "    return (bev/max_intensity).clip(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_model(model_name=\"efficient-b3\", IN_CHANNEL=3, NUM_CLASSES=2, SIZE=336):\n",
    "    model = model_iMet(model_name, IN_CHANNEL, NUM_CLASSES, SIZE)\n",
    "    \n",
    "    # Optional, for multi GPU training and inference\n",
    "    # model = nn.DataParallel(model)\n",
    "    return model\n",
    "\n",
    "# We weigh the loss for the 0 class lower to account for (some of) the big class imbalance.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]\n",
    "class_weights = torch.from_numpy(np.array([0.2] + [1.0]*len(classes), dtype=np.float32))\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "test_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, test_batch_size, shuffle=False, num_workers=os.cpu_count()*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with efficientnet-b3 and seresnext_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filename_b3 = \"unet_checkpoint_b3/unet_checkpoint_b3.pth\"\n",
    "checkpoint_filepath_b3 = os.path.join(\"/media/jionie/my_disk/Kaggle/Lyft/model/unet/\", checkpoint_filename_b3)\n",
    "\n",
    "model_b3 = get_unet_model(model_name=\"efficientnet-b3\", IN_CHANNEL=3, NUM_CLASSES=len(classes)+1, SIZE=SIZE)\n",
    "model_b3.load_pretrain(checkpoint_filepath_b3)\n",
    "model_b3 = model_b3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filename_seresnext101 = \"unet_checkpoint_seresnext101/seresnext101_unet_checkpoint.pth\"\n",
    "checkpoint_filepath_seresnext101 = os.path.join(\"/media/jionie/my_disk/Kaggle/Lyft/model/unet/\", checkpoint_filename_seresnext101)\n",
    "\n",
    "model_seresnext101 = get_unet_model(model_name=\"seresnext101\", IN_CHANNEL=3, NUM_CLASSES=len(classes)+1, SIZE=SIZE)\n",
    "model_seresnext101.load_pretrain(checkpoint_filepath_seresnext101)\n",
    "model_seresnext101 = model_seresnext101.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filename_unet_reference = \"unet_reference/seresnext101_unet_checkpoint.pth\"\n",
    "checkpoint_filepath_unet_reference = os.path.join(\"/media/jionie/my_disk/Kaggle/Lyft/model/unet/\", checkpoint_filename_seresnext101)\n",
    "\n",
    "model_seresnext101 = get_unet_model(model_name=\"seresnext101\", IN_CHANNEL=3, NUM_CLASSES=len(classes)+1, SIZE=SIZE)\n",
    "model_seresnext101.load_pretrain(checkpoint_filepath_seresnext101)\n",
    "model_seresnext101 = model_seresnext101.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jionie/py3env/lib/python3.6/site-packages/torch/nn/functional.py:2622: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = []\n",
    "detection_boxes = []\n",
    "detection_scores = []\n",
    "detection_classes = []\n",
    "\n",
    "print(\"Inference\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for test_batch_i, (X, sample_ids) in enumerate(test_dataloader):\n",
    "\n",
    "        model_b3.eval()\n",
    "        model_seresnext101.eval()\n",
    "\n",
    "        sample_tokens.extend(sample_ids)\n",
    "\n",
    "        X = X.to(device).float()  # [N, 3, H, W]\n",
    "        prediction_b3, _ = model_b3(X)  # [N, 2, H, W]\n",
    "        prediction_seresnext101, _ = model_seresnext101(X)  # [N, 2, H, W]\n",
    "        \n",
    "        prediction = (prediction_b3 + prediction_seresnext101) / 2\n",
    "        \n",
    "        prediction = F.softmax(prediction, dim=1).cpu().numpy()\n",
    "        \n",
    "        predictions = np.round(prediction*255).astype(np.uint8)\n",
    "        \n",
    "        # Get probabilities for non-background\n",
    "        predictions_non_class0 = 255 - predictions[:, 0]\n",
    "        \n",
    "        predictions_opened = np.zeros((predictions_non_class0.shape), dtype=np.uint8)\n",
    "\n",
    "        for i, p in enumerate(predictions_non_class0):\n",
    "            thresholded_p = (p > background_threshold).astype(np.uint8)\n",
    "            predictions_opened[i] = cv2.morphologyEx(thresholded_p, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "            sample_boxes,sample_detection_scores,sample_detection_classes = calc_detection_box(predictions_opened[i], predictions[i])\n",
    "        \n",
    "            detection_boxes.append(np.array(sample_boxes))\n",
    "            detection_scores.append(sample_detection_scores)\n",
    "            detection_classes.append(sample_detection_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of boxes: 686701\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total amount of boxes:\", np.sum([len(x) for x in detection_boxes]))\n",
    "    \n",
    "\n",
    "# Visualize the boxes in the first sample\n",
    "t = np.zeros_like(predictions_opened[0])\n",
    "for sample_boxes in detection_boxes[0]:\n",
    "    box_pix = np.int0(sample_boxes)\n",
    "    cv2.drawContours(t,[box_pix],0,(255),2)\n",
    "plt.imshow(t)\n",
    "plt.show()\n",
    "\n",
    "# Visualize their probabilities\n",
    "plt.hist(detection_scores[0], bins=20)\n",
    "plt.xlabel(\"Detection Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_box3ds = []\n",
    "\n",
    "# print(\"Generating world boxes\")\n",
    "\n",
    "# # This could use some refactoring..\n",
    "# for (sample_token, sample_boxes, sample_detection_scores, sample_detection_class) in zip(sample_tokens, detection_boxes, detection_scores, detection_classes):\n",
    "\n",
    "#     sample_boxes = sample_boxes.reshape(-1, 2) # (N, 4, 2) -> (N*4, 2)\n",
    "#     sample_boxes = sample_boxes.transpose(1,0) # (N*4, 2) -> (2, N*4)\n",
    "\n",
    "#     # Add Z dimension\n",
    "#     sample_boxes = np.vstack((sample_boxes, np.zeros(sample_boxes.shape[1]),)) # (2, N*4) -> (3, N*4)\n",
    "\n",
    "#     sample = level5data.get(\"sample\", sample_token)\n",
    "#     sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "#     lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "#     lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "#     ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "#     ego_translation = np.array(ego_pose['translation'])\n",
    "\n",
    "#     global_from_car = transform_matrix(ego_pose['translation'],\n",
    "#                                        Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "#     car_from_voxel = np.linalg.inv(create_transformation_matrix_to_voxel_space(bev_shape, voxel_size, (0, 0, z_offset)))\n",
    "\n",
    "\n",
    "#     global_from_voxel = np.dot(global_from_car, car_from_voxel)\n",
    "#     sample_boxes = transform_points(sample_boxes, global_from_voxel)\n",
    "\n",
    "#     # We don't know at where the boxes are in the scene on the z-axis (up-down), let's assume all of them are at\n",
    "#     # the same height as the ego vehicle.\n",
    "#     sample_boxes[2,:] = ego_pose[\"translation\"][2]\n",
    "\n",
    "\n",
    "#     # (3, N*4) -> (N, 4, 3)\n",
    "#     sample_boxes = sample_boxes.transpose(1,0).reshape(-1, 4, 3)\n",
    "\n",
    "#     # box_height, width, len\n",
    "#     box_height = np.array([class_height[cls] for cls in sample_detection_class])\n",
    "#     box_width = np.array([class_width[cls] for cls in sample_detection_class])\n",
    "#     box_len = np.array([class_len[cls] for cls in sample_detection_class])\n",
    "\n",
    "#     # Note: Each of these boxes describes the ground corners of a 3D box.\n",
    "#     # To get the center of the box in 3D, we'll have to add half the height to it.\n",
    "#     sample_boxes_centers = sample_boxes.mean(axis=1)\n",
    "#     sample_boxes_centers[:, 2] += box_height/2\n",
    "\n",
    "#     # Width and height is arbitrary - we don't know what way the vehicles are pointing from our prediction segmentation\n",
    "#     # It doesn't matter for evaluation, so no need to worry about that here.\n",
    "#     # Note: We scaled our targets to be 0.8 the actual size, we need to adjust for that\n",
    "#     sample_lengths = np.linalg.norm(sample_boxes[:,0,:] - sample_boxes[:,1,:], axis=1) * 1/box_scale\n",
    "#     sample_widths = np.linalg.norm(sample_boxes[:,1,:] - sample_boxes[:,2,:], axis=1) * 1/box_scale\n",
    "    \n",
    "#     sample_boxes_dimensions = np.zeros_like(sample_boxes_centers) \n",
    "#     sample_boxes_dimensions[:,0] = sample_widths\n",
    "#     sample_boxes_dimensions[:,1] = sample_lengths\n",
    "#     sample_boxes_dimensions[:,2] = box_height\n",
    "\n",
    "#     for i in range(len(sample_boxes)):\n",
    "#         translation = sample_boxes_centers[i]\n",
    "#         size = sample_boxes_dimensions[i]\n",
    "#         class_name = sample_detection_class[i]\n",
    "#         ego_distance = float(np.linalg.norm(ego_translation - translation))\n",
    "    \n",
    "        \n",
    "#         # Determine the rotation of the box\n",
    "#         v = (sample_boxes[i,0] - sample_boxes[i,1])\n",
    "#         v /= np.linalg.norm(v)\n",
    "#         r = R.from_dcm([\n",
    "#             [v[0], -v[1], 0],\n",
    "#             [v[1],  v[0], 0],\n",
    "#             [   0,     0, 1],\n",
    "#         ])\n",
    "#         quat = r.as_quat()\n",
    "#         # XYZW -> WXYZ order of elements\n",
    "#         quat = quat[[3,0,1,2]]\n",
    "        \n",
    "#         detection_score = float(sample_detection_scores[i])\n",
    "\n",
    "        \n",
    "#         box3d = Box3D(\n",
    "#             sample_token=sample_token,\n",
    "#             translation=list(translation),\n",
    "#             size=list(size),\n",
    "#             rotation=list(quat),\n",
    "#             name=class_name,\n",
    "#             score=detection_score\n",
    "#         )\n",
    "#         pred_box3ds.append(box3d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting moviepy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/b8005c49fd3975a9660dfd648292bb043a5d811fe17339e8f7b79f3ec796/moviepy-1.0.1.tar.gz (373kB)\n",
      "\u001b[K    100% |████████████████████████████████| 378kB 13.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator<5.0,>=4.0.2 in /home/jionie/py3env/lib/python3.6/site-packages (from moviepy) (4.3.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/jionie/py3env/lib/python3.6/site-packages (from moviepy) (4.31.1)\n",
      "Requirement already satisfied: numpy in /home/jionie/py3env/lib/python3.6/site-packages (from moviepy) (1.16.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /home/jionie/py3env/lib/python3.6/site-packages (from moviepy) (2.21.0)\n",
      "Collecting proglog<=1.0.0 (from moviepy)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ab/4cb19b578e1364c0b2d6efd6521a8b4b4e5c4ae6528041d31a2a951dd991/proglog-0.1.9.tar.gz\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/jionie/py3env/lib/python3.6/site-packages (from moviepy) (2.5.0)\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/12/01126a2fb737b23461d7dadad3b8abd51ad6210f979ff05c6fa9812dfbbe/imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl (22.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 22.2MB 3.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/jionie/py3env/lib/python3.6/site-packages (from requests<3.0,>=2.8.1->moviepy) (2018.11.29)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/jionie/py3env/lib/python3.6/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/jionie/py3env/lib/python3.6/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jionie/py3env/lib/python3.6/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.0.4)\n",
      "Requirement already satisfied: pillow in /home/jionie/py3env/lib/python3.6/site-packages (from imageio<3.0,>=2.5->moviepy) (5.4.1)\n",
      "Building wheels for collected packages: moviepy, proglog\n",
      "  Building wheel for moviepy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jionie/.cache/pip/wheels/a3/3c/07/45afe2bd5dbd3f935f445545d645f0f8c05d48340136367d45\n",
      "  Building wheel for proglog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jionie/.cache/pip/wheels/65/56/60/1d0306a8d90b188af393c1812ddb502a8821b70917f82dcc00\n",
      "Successfully built moviepy proglog\n",
      "Installing collected packages: proglog, imageio-ffmpeg, moviepy\n",
      "Successfully installed imageio-ffmpeg-0.3.0 moviepy-1.0.1 proglog-0.1.9\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_in_image(box, intrinsic, image_size) -> bool:\n",
    "    \"\"\"Check if a box is visible inside an image without accounting for occlusions.\n",
    "    Args:\n",
    "        box: The box to be checked.\n",
    "        intrinsic: <float: 3, 3>. Intrinsic camera matrix.\n",
    "        image_size: (width, height)\n",
    "        vis_level: One of the enumerations of <BoxVisibility>.\n",
    "    Returns: True if visibility condition is satisfied.\n",
    "    \"\"\"\n",
    "\n",
    "    corners_3d = box.corners()\n",
    "    corners_img = view_points(corners_3d, intrinsic, normalize=True)[:2, :]\n",
    "\n",
    "    visible = np.logical_and(corners_img[0, :] > 0, corners_img[0, :] < image_size[0])\n",
    "    visible = np.logical_and(visible, corners_img[1, :] < image_size[1])\n",
    "    visible = np.logical_and(visible, corners_img[1, :] > 0)\n",
    "    visible = np.logical_and(visible, corners_3d[2, :] > 1)\n",
    "\n",
    "    in_front = corners_3d[2, :] > 0.1  # True if a corner is at least 0.1 meter in front of the camera.\n",
    "\n",
    "    return any(visible) and all(in_front)\n",
    "\n",
    "all_pred_fn = []\n",
    "\n",
    "def viz_unet(sample_token, boxes): \n",
    "\n",
    "    sample = level5data.get(\"sample\", sample_token)\n",
    "\n",
    "    sample_camera_token = sample[\"data\"][\"CAM_FRONT\"]\n",
    "    camera_data = level5data.get(\"sample_data\", sample_camera_token)\n",
    "    # camera_filepath = level5data.get_sample_data_path(sample_camera_token)\n",
    "\n",
    "    ego_pose = level5data.get(\"ego_pose\", camera_data[\"ego_pose_token\"])\n",
    "    calibrated_sensor = level5data.get(\"calibrated_sensor\", camera_data[\"calibrated_sensor_token\"])\n",
    "    data_path_, _, camera_intrinsic = level5data.get_sample_data(sample_camera_token)\n",
    "\n",
    "    data_path = Path('/media/jionie/my_disk/Kaggle/Lyft/input/3d-object-detection-for-autonomous-vehicles/test_root/images/' \\\n",
    "                     + str(data_path_).split('/')[-1])\n",
    "    data = Image.open(data_path)\n",
    "#     os.path.join(self.test_data_folder,f\"{sample_token}_input.png\")\n",
    "    _, axis = plt.subplots(1, 1, figsize=(9, 9))\n",
    "    \n",
    "    for i,box in enumerate(boxes):\n",
    "\n",
    "        # Move box to ego vehicle coord system\n",
    "        box.translate(-np.array(ego_pose[\"translation\"]))\n",
    "        box.rotate(Quaternion(ego_pose[\"rotation\"]).inverse)\n",
    "\n",
    "        # Move box to sensor coord system\n",
    "        box.translate(-np.array(calibrated_sensor[\"translation\"]))\n",
    "        box.rotate(Quaternion(calibrated_sensor[\"rotation\"]).inverse)\n",
    "\n",
    "        if box_in_image(box,camera_intrinsic,np.array(data).shape):            \n",
    "            box.render(axis,camera_intrinsic,normalize=True)\n",
    "\n",
    "    axis.imshow(data)\n",
    "    all_pred_fn.append(f'./cam_viz/cam_preds_{sample_token}.jpg')\n",
    "    plt.savefig(all_pred_fn[-1])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3072978b428d41148f75d3ad8b07b979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:   0%|          | 0/128 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video model_preds_1.mp4.\n",
      "Moviepy - Writing video model_preds_1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready model_preds_1.mp4\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('./cam_viz',exist_ok=True)\n",
    "\n",
    "from moviepy.editor import ImageSequenceClip \n",
    "from lyft_dataset_sdk.eval.detection.mAP_evaluation import Box3D, recall_precision\n",
    "import shutil\n",
    "\n",
    "pred_box3ds = []\n",
    "\n",
    "max_frames = 128\n",
    "vid_count = 0\n",
    "processed_samples = 0\n",
    "for (sample_token, sample_boxes, sample_detection_scores, sample_detection_class) in tqdm_notebook(zip(sample_tokens, detection_boxes, detection_scores, detection_classes), total=len(sample_tokens)):\n",
    "    processed_samples += 1\n",
    "    sample_boxes = sample_boxes.reshape(-1, 2) # (N, 4, 2) -> (N*4, 2)\n",
    "    sample_boxes = sample_boxes.transpose(1,0) # (N*4, 2) -> (2, N*4)\n",
    "\n",
    "    # Add Z dimension\n",
    "    sample_boxes = np.vstack((sample_boxes, np.zeros(sample_boxes.shape[1]),)) # (2, N*4) -> (3, N*4)\n",
    "\n",
    "    sample = level5data.get(\"sample\", sample_token)\n",
    "    sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "    lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "    lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "    ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "    ego_translation = np.array(ego_pose['translation'])\n",
    "\n",
    "    global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                       Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "    car_from_voxel = np.linalg.inv(create_transformation_matrix_to_voxel_space(bev_shape, voxel_size, (0, 0, z_offset)))\n",
    "\n",
    "\n",
    "    global_from_voxel = np.dot(global_from_car, car_from_voxel)\n",
    "    sample_boxes = transform_points(sample_boxes, global_from_voxel)\n",
    "\n",
    "    # We don't know at where the boxes are in the scene on the z-axis (up-down), let's assume all of them are at\n",
    "    # the same height as the ego vehicle.\n",
    "    sample_boxes[2,:] = ego_pose[\"translation\"][2]\n",
    "\n",
    "\n",
    "    # (3, N*4) -> (N, 4, 3)\n",
    "    sample_boxes = sample_boxes.transpose(1,0).reshape(-1, 4, 3)\n",
    "\n",
    "#     box_height = 1.75\n",
    "    box_height = np.array([class_height[cls] for cls in sample_detection_class])\n",
    "\n",
    "    # Note: Each of these boxes describes the ground corners of a 3D box.\n",
    "    # To get the center of the box in 3D, we'll have to add half the height to it.\n",
    "    sample_boxes_centers = sample_boxes.mean(axis=1)\n",
    "    sample_boxes_centers[:,2] += box_height/2\n",
    "\n",
    "    # Width and height is arbitrary - we don't know what way the vehicles are pointing from our prediction segmentation\n",
    "    # It doesn't matter for evaluation, so no need to worry about that here.\n",
    "    # Note: We scaled our targets to be 0.8 the actual size, we need to adjust for that\n",
    "    sample_lengths = np.linalg.norm(sample_boxes[:,0,:] - sample_boxes[:,1,:], axis=1) * 1/box_scale\n",
    "    sample_widths = np.linalg.norm(sample_boxes[:,1,:] - sample_boxes[:,2,:], axis=1) * 1/box_scale\n",
    "    \n",
    "    sample_boxes_dimensions = np.zeros_like(sample_boxes_centers) \n",
    "    sample_boxes_dimensions[:,0] = sample_widths\n",
    "    sample_boxes_dimensions[:,1] = sample_lengths\n",
    "    sample_boxes_dimensions[:,2] = box_height\n",
    "    \n",
    "    temp = []\n",
    "    for i in range(len(sample_boxes)):\n",
    "        translation = sample_boxes_centers[i]\n",
    "        size = sample_boxes_dimensions[i]\n",
    "        class_name = sample_detection_class[i]\n",
    "        ego_distance = float(np.linalg.norm(ego_translation - translation))\n",
    "    \n",
    "        \n",
    "        # Determine the rotation of the box\n",
    "        v = (sample_boxes[i,0] - sample_boxes[i,1])\n",
    "        v /= np.linalg.norm(v)\n",
    "        r = R.from_dcm([\n",
    "            [v[0], -v[1], 0],\n",
    "            [v[1],  v[0], 0],\n",
    "            [   0,     0, 1],\n",
    "        ])\n",
    "        quat = r.as_quat()\n",
    "        # XYZW -> WXYZ order of elements\n",
    "        quat = quat[[3,0,1,2]]\n",
    "        \n",
    "        detection_score = float(sample_detection_scores[i])\n",
    "\n",
    "        \n",
    "        box3d = Box(\n",
    "            token=sample_token,\n",
    "            center=list(translation),\n",
    "            size=list(size),\n",
    "            orientation=Quaternion(quat),\n",
    "            name=class_name,\n",
    "            score=detection_score\n",
    "        )\n",
    "        \n",
    "        temp.append(box3d)\n",
    "        box3d = Box3D(\n",
    "            sample_token=sample_token,\n",
    "            translation=list(translation),\n",
    "            size=list(size),\n",
    "            rotation=list(quat),\n",
    "            name=class_name,\n",
    "            score=detection_score\n",
    "        )\n",
    "        pred_box3ds.append(box3d)\n",
    "        \n",
    "#     https://github.com/Zulko/moviepy/issues/903\n",
    "    if vid_count < 1:\n",
    "        viz_unet(sample_token,temp)\n",
    "        if processed_samples==max_frames:\n",
    "            os.makedirs('./cam_viz',exist_ok=True)\n",
    "            processed_samples = 0\n",
    "            vid_count += 1        \n",
    "            new_clip = ImageSequenceClip(all_pred_fn,fps=5)\n",
    "            all_pred_fn = []\n",
    "            new_clip.write_videofile(f\"model_preds_{vid_count}.mp4\") \n",
    "            shutil.rmtree('./cam_viz')\n",
    "            del new_clip\n",
    "            gc.collect()\n",
    "            os.makedirs('./cam_viz',exist_ok=True)\n",
    "#         os.system('rm -rf ./cam_viz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = {}\n",
    "\n",
    "print(\"Generating final csv\")\n",
    "\n",
    "for i in range(len(pred_box3ds)):\n",
    "\n",
    "    if (i % 50 == 0):\n",
    "        print(\"Generating \", i, \" of \", len(pred_box3ds), \" samples.\")\n",
    "\n",
    "    # yaw = -np.arctan2(pred_box3ds[i].rotation[2], pred_box3ds[i].rotation[0])\n",
    "    yaw = 2*np.arccos(pred_box3ds[i].rotation[0])\n",
    "\n",
    "    pred =  str(pred_box3ds[i].score/255) + ' ' + str(pred_box3ds[i].center_x)  + ' '  + str(pred_box3ds[i].center_y) + ' '  + str(pred_box3ds[i].center_z) + ' '  + \\\n",
    "        str(pred_box3ds[i].width) + ' ' + str(pred_box3ds[i].length) + ' '  + str(pred_box3ds[i].height) + ' ' + \\\n",
    "         str(yaw) + ' ' + str(pred_box3ds[i].name) + ' ' \n",
    "        \n",
    "    if pred_box3ds[i].sample_token in sub.keys():     \n",
    "        sub[pred_box3ds[i].sample_token] += pred\n",
    "    else:\n",
    "        sub[pred_box3ds[i].sample_token] = pred        \n",
    "    \n",
    "sample_sub = pd.read_csv('/media/jionie/my_disk/Kaggle/Lyft/input/3d-object-detection-for-autonomous-vehicles/test_root/sample_submission.csv')\n",
    "for token in set(sample_sub.Id.values).difference(sub.keys()):\n",
    "    print(token)\n",
    "    sub[token] = ''\n",
    "\n",
    "sub = pd.DataFrame(list(sub.items()))\n",
    "sub.columns = sample_sub.columns\n",
    "sub.to_csv('lyft3d_pred.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3env",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
