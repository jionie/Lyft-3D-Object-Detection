{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updates\n",
    "\n",
    "- I have added code that does model prediction visualization by putting it in camera frame. I found this to be more intuitive than visualizing it in lidar frame.\n",
    "I hope you find it useful.\n",
    "\n",
    "If you check out the reference model in [lyft devkit](https://github.com/lyft/nuscenes-devkit/tree/master/notebooks), you will find that they are using map masks for training UNET. The map masks are first extracted around the corresponding ego region and used as 3 additional channels. This seems to be give some improvement in lb score.\n",
    "\n",
    "In this notebook i do inference using such a trained trained model.\n",
    "I have extracted the ego centered maps and made a test dataset. You can find it [here](https://www.kaggle.com/meaninglesslives/lyft3d-mask-test-data). It takes a long time to compute on test set, so i am sharing it here :-D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the Lyft SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting moviepy\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/b8005c49fd3975a9660dfd648292bb043a5d811fe17339e8f7b79f3ec796/moviepy-1.0.1.tar.gz (373kB)\r\n",
      "\u001b[K     |████████████████████████████████| 378kB 9.5MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: decorator<5.0,>=4.0.2 in /opt/conda/lib/python3.6/site-packages (from moviepy) (4.4.0)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.6/site-packages (from moviepy) (4.36.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from moviepy) (1.16.4)\r\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.6/site-packages (from moviepy) (2.22.0)\r\n",
      "Collecting proglog<=1.0.0 (from moviepy)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ab/4cb19b578e1364c0b2d6efd6521a8b4b4e5c4ae6528041d31a2a951dd991/proglog-0.1.9.tar.gz\r\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.6/site-packages (from moviepy) (2.5.0)\r\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/12/01126a2fb737b23461d7dadad3b8abd51ad6210f979ff05c6fa9812dfbbe/imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl (22.2MB)\r\n",
      "\u001b[K     |████████████████████████████████| 22.2MB 40.2MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.24.2)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0,>=2.8.1->moviepy) (2019.9.11)\r\n",
      "Building wheels for collected packages: moviepy, proglog\r\n",
      "  Building wheel for moviepy (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for moviepy: filename=moviepy-1.0.1-cp36-none-any.whl size=110786 sha256=4c0922030d2cf992df93751237d8218ec378fb512754c437660c404f9db4815a\r\n",
      "  Stored in directory: /tmp/.cache/pip/wheels/a3/3c/07/45afe2bd5dbd3f935f445545d645f0f8c05d48340136367d45\r\n",
      "  Building wheel for proglog (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for proglog: filename=proglog-0.1.9-cp36-none-any.whl size=6148 sha256=8187d79d914aa8b0c1e1d324046d517b6a7afcf273af20b335ded30a0f6eb1e6\r\n",
      "  Stored in directory: /tmp/.cache/pip/wheels/65/56/60/1d0306a8d90b188af393c1812ddb502a8821b70917f82dcc00\r\n",
      "Successfully built moviepy proglog\r\n",
      "Installing collected packages: proglog, imageio-ffmpeg, moviepy\r\n",
      "Successfully installed imageio-ffmpeg-0.3.0 moviepy-1.0.1 proglog-0.1.9\r\n"
     ]
    }
   ],
   "source": [
    "!pip install lyft-dataset-sdk -q\n",
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces\n",
    "# even more threads which would lead to a lot of context switching, slowing things down a lot.\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n",
    "\n",
    "import time\n",
    "from lyft_dataset_sdk.utils.map_mask import MapMask\n",
    "from pathlib import Path\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset,LyftDatasetExplorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyft SDK requires creating a link to input folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/test_images images\n",
    "!ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/test_maps maps\n",
    "!ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/test_lidar lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 13.0 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 5.3 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]\n",
    "train_dataset = LyftDataset(data_path='.', json_path='../input/3d-object-detection-for-autonomous-vehicles/train_data', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the mean height of all categories\n",
    "We can use the mean height instead of blindly using 1.75m for all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category stats\n",
      "animal                      n=  186, width= 0.36±0.12, len= 0.73±0.19, height= 0.51±0.16, lw_aspect= 2.16±0.56\n",
      "bicycle                     n=20928, width= 0.63±0.24, len= 1.76±0.29, height= 1.44±0.37, lw_aspect= 3.20±1.17\n",
      "bus                         n= 8729, width= 2.96±0.24, len=12.34±3.41, height= 3.44±0.31, lw_aspect= 4.17±1.10\n",
      "car                         n=534911, width= 1.93±0.16, len= 4.76±0.53, height= 1.72±0.24, lw_aspect= 2.47±0.22\n",
      "emergency_vehicle           n=  132, width= 2.45±0.43, len= 6.52±1.44, height= 2.39±0.59, lw_aspect= 2.66±0.28\n",
      "motorcycle                  n=  818, width= 0.96±0.20, len= 2.35±0.22, height= 1.59±0.16, lw_aspect= 2.53±0.50\n",
      "other_vehicle               n=33376, width= 2.79±0.30, len= 8.20±1.71, height= 3.23±0.50, lw_aspect= 2.93±0.53\n",
      "pedestrian                  n=24935, width= 0.77±0.14, len= 0.81±0.17, height= 1.78±0.16, lw_aspect= 1.06±0.20\n",
      "truck                       n=14164, width= 2.84±0.32, len=10.24±4.09, height= 3.44±0.62, lw_aspect= 3.56±1.25\n"
     ]
    }
   ],
   "source": [
    "train_dataset.list_categories()\n",
    "del train_dataset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file instance.json missing, using empty list\n",
      "JSON file sample_annotation.json missing, using empty list\n",
      "9 category,\n",
      "17 attribute,\n",
      "4 visibility,\n",
      "0 instance,\n",
      "8 sensor,\n",
      "168 calibrated_sensor,\n",
      "219744 ego_pose,\n",
      "218 log,\n",
      "218 scene,\n",
      "27468 sample,\n",
      "219744 sample_data,\n",
      "0 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 3.0 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 1.6 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "class_heights = {'animal':0.51,'bicycle':1.44,'bus':3.44,'car':1.72,'emergency_vehicle':2.39,'motorcycle':1.59,\n",
    "                'other_vehicle':3.23,'pedestrian':1.78,'truck':3.44}\n",
    "level5data = LyftDataset(data_path='.', json_path='../input/3d-object-detection-for-autonomous-vehicles/test_data', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_boxes_to_car_space(boxes, ego_pose):\n",
    "    \"\"\"\n",
    "    Move boxes from world space to car space.\n",
    "    Note: mutates input boxes.\n",
    "    \"\"\"\n",
    "    translation = -np.array(ego_pose['translation'])\n",
    "    rotation = Quaternion(ego_pose['rotation']).inverse\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Bring box to car space\n",
    "        box.translate(translation)\n",
    "        box.rotate(rotation)\n",
    "        \n",
    "def scale_boxes(boxes, factor):\n",
    "    \"\"\"\n",
    "    Note: mutates input boxes\n",
    "    \"\"\"\n",
    "    for box in boxes:\n",
    "        box.wlh = box.wlh * factor\n",
    "\n",
    "def draw_boxes(im, voxel_size, boxes, classes, z_offset=0.0):\n",
    "    for box in boxes:\n",
    "        # We only care about the bottom corners\n",
    "        corners = box.bottom_corners()\n",
    "        corners_voxel = car_to_voxel_coords(corners, im.shape, voxel_size, z_offset).transpose(1,0)\n",
    "        corners_voxel = corners_voxel[:,:2] # Drop z coord\n",
    "\n",
    "        class_color = classes.index(box.name) + 1\n",
    "        \n",
    "        if class_color == 0:\n",
    "            raise Exception(\"Unknown class: {}\".format(box.name))\n",
    "\n",
    "        cv2.drawContours(im, np.int0([corners_voxel]), 0, (class_color, class_color, class_color), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hyperparameters we'll need to define for the system\n",
    "voxel_size = (0.4, 0.4, 1.5)\n",
    "z_offset = -2.0\n",
    "bev_shape = (336, 336, 3)\n",
    "\n",
    "# We scale down each box so they are more separated when projected into our coarse voxel space.\n",
    "box_scale = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [(level5data.get('sample', record['first_sample_token'])['timestamp'], record) for record in level5data.scene]\n",
    "\n",
    "entries = []\n",
    "\n",
    "for start_time, record in sorted(records):\n",
    "    start_time = level5data.get('sample', record['first_sample_token'])['timestamp'] / 1000000\n",
    "\n",
    "    token = record['token']\n",
    "    name = record['name']\n",
    "    date = datetime.utcfromtimestamp(start_time)\n",
    "    host = \"-\".join(record['name'].split(\"-\")[:2])\n",
    "    first_sample_token = record[\"first_sample_token\"]\n",
    "\n",
    "    entries.append((host, name, date, token, first_sample_token))\n",
    "            \n",
    "df = pd.DataFrame(entries, columns=[\"host\", \"scene_name\", \"date\", \"scene_token\", \"first_sample_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host</th>\n",
       "      <th>scene_name</th>\n",
       "      <th>date</th>\n",
       "      <th>scene_token</th>\n",
       "      <th>first_sample_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>host-a007</td>\n",
       "      <td>host-a007-lidar0-1230678335199240106-123067836...</td>\n",
       "      <td>2019-01-04 23:05:35.302051</td>\n",
       "      <td>582583077b5db62b9b95d780c7fb3214f2fb6680ec61ef...</td>\n",
       "      <td>57b51d24fad1c39065f80c9f769b3ff8d29e17e0cf92f5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>host-a007</td>\n",
       "      <td>host-a007-lidar0-1230931253199029066-123093127...</td>\n",
       "      <td>2019-01-07 21:20:53.301923</td>\n",
       "      <td>795a318208bdc612f92eabe3af3102f9e55db4b6c1e44e...</td>\n",
       "      <td>780168b66b14e7f826b365d7f5f0ec602d70fc5df4edb9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>host-a007</td>\n",
       "      <td>host-a007-lidar0-1230939239197974066-123093926...</td>\n",
       "      <td>2019-01-07 23:33:59.300673</td>\n",
       "      <td>84ae53a19295800de1565ea5e61ef0d5e3d938a4b0601c...</td>\n",
       "      <td>99e78fc0682700b8be3635ef06231d813951d038e48795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>host-a009</td>\n",
       "      <td>host-a009-lidar0-1231184014198521956-123118403...</td>\n",
       "      <td>2019-01-10 19:33:34.301170</td>\n",
       "      <td>225300b10634526aafdce9bb3e00e863b24663e6766041...</td>\n",
       "      <td>c88dfa9d87d41079250dd7488f9ee3721dfda7e5b45563...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>host-a008</td>\n",
       "      <td>host-a008-lidar0-1231272360198562866-123127238...</td>\n",
       "      <td>2019-01-11 20:06:00.301206</td>\n",
       "      <td>07b333dad30191a7d3048919c420083a7f53296b885038...</td>\n",
       "      <td>2299492f339a64c15d192e1b9ac3836917fb77660a4d72...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        host                                         scene_name  \\\n",
       "0  host-a007  host-a007-lidar0-1230678335199240106-123067836...   \n",
       "1  host-a007  host-a007-lidar0-1230931253199029066-123093127...   \n",
       "2  host-a007  host-a007-lidar0-1230939239197974066-123093926...   \n",
       "3  host-a009  host-a009-lidar0-1231184014198521956-123118403...   \n",
       "4  host-a008  host-a008-lidar0-1231272360198562866-123127238...   \n",
       "\n",
       "                        date  \\\n",
       "0 2019-01-04 23:05:35.302051   \n",
       "1 2019-01-07 21:20:53.301923   \n",
       "2 2019-01-07 23:33:59.300673   \n",
       "3 2019-01-10 19:33:34.301170   \n",
       "4 2019-01-11 20:06:00.301206   \n",
       "\n",
       "                                         scene_token  \\\n",
       "0  582583077b5db62b9b95d780c7fb3214f2fb6680ec61ef...   \n",
       "1  795a318208bdc612f92eabe3af3102f9e55db4b6c1e44e...   \n",
       "2  84ae53a19295800de1565ea5e61ef0d5e3d938a4b0601c...   \n",
       "3  225300b10634526aafdce9bb3e00e863b24663e6766041...   \n",
       "4  07b333dad30191a7d3048919c420083a7f53296b885038...   \n",
       "\n",
       "                                  first_sample_token  \n",
       "0  57b51d24fad1c39065f80c9f769b3ff8d29e17e0cf92f5...  \n",
       "1  780168b66b14e7f826b365d7f5f0ec602d70fc5df4edb9...  \n",
       "2  99e78fc0682700b8be3635ef06231d813951d038e48795...  \n",
       "3  c88dfa9d87d41079250dd7488f9ee3721dfda7e5b45563...  \n",
       "4  2299492f339a64c15d192e1b9ac3836917fb77660a4d72...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0518b492900f47ee87a74a746c9a4314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=218), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of tokens= 27468\n"
     ]
    }
   ],
   "source": [
    "# sample_sub = pd.read_csv('../input/3d-object-detection-for-autonomous-vehicles/sample_submission.csv')\n",
    "all_sample_tokens,scene_len = [],[]\n",
    "for sample_token in tqdm_notebook(df.first_sample_token.values):\n",
    "    i = 0\n",
    "    while sample_token:\n",
    "        all_sample_tokens.append(sample_token)\n",
    "        sample = level5data.get(\"sample\", sample_token)\n",
    "        sample_token = sample[\"next\"]\n",
    "        i += 1\n",
    "    scene_len.append(i)\n",
    "#     print(len(all_sample_tokens[-1]))\n",
    "    \n",
    "print('Total number of tokens=',len(all_sample_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "test_data_folder = '../input/lyft3d-mask-test-data/test_data/test_data'\n",
    "\n",
    "class BEVImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sample_token,test_data_folder):\n",
    "\n",
    "        self.sample_token = sample_token\n",
    "        self.test_data_folder = test_data_folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_token)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_token = self.sample_token[idx]\n",
    "        \n",
    "#         sample_token = input_filepath.split(\"/\")[-1].replace(\"_input.png\",\"\")\n",
    "        \n",
    "        input_filepath = os.path.join(test_data_folder,f\"{sample_token}_input.png\")\n",
    "\n",
    "        map_filepath = os.path.join(test_data_folder,f\"{sample_token}_map.png\")\n",
    "        \n",
    "        \n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "#         print(im.shape,map_im.shape)\n",
    "        im = np.concatenate((im, map_im), axis=2)\n",
    "        \n",
    "        im = im.astype(np.float32)/255\n",
    "        \n",
    "        im = torch.from_numpy(im.transpose(2,0,1))\n",
    "        \n",
    "        return im, sample_token\n",
    "\n",
    "    \n",
    "# input_filepaths = sorted(glob.glob(os.path.join(test_data_folder, \"*_input.png\")))\n",
    "# map_filepaths = sorted(glob.glob(os.path.join(test_data_folder, \"*_map.png\")))\n",
    "\n",
    "test_dataset = BEVImageDataset(all_sample_tokens,test_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation was copied from https://github.com/jvanvugt/pytorch-unet, it is MIT licensed.\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a U-net fully convolutional neural network, we create a network that is less deep and with only half the amount of filters compared to the original U-net paper implementation. We do this to keep training and inference time low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_model(in_channels=6, num_output_classes=2):\n",
    "    model = UNet(in_channels=in_channels, n_classes=num_output_classes, wf=5, depth=4, padding=True, up_mode='upsample')\n",
    "    \n",
    "    # Optional, for multi GPU training and inference\n",
    "    model = nn.DataParallel(model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_in_image(box, intrinsic, image_size) -> bool:\n",
    "    \"\"\"Check if a box is visible inside an image without accounting for occlusions.\n",
    "    Args:\n",
    "        box: The box to be checked.\n",
    "        intrinsic: <float: 3, 3>. Intrinsic camera matrix.\n",
    "        image_size: (width, height)\n",
    "        vis_level: One of the enumerations of <BoxVisibility>.\n",
    "    Returns: True if visibility condition is satisfied.\n",
    "    \"\"\"\n",
    "\n",
    "    corners_3d = box.corners()\n",
    "    corners_img = view_points(corners_3d, intrinsic, normalize=True)[:2, :]\n",
    "\n",
    "    visible = np.logical_and(corners_img[0, :] > 0, corners_img[0, :] < image_size[0])\n",
    "    visible = np.logical_and(visible, corners_img[1, :] < image_size[1])\n",
    "    visible = np.logical_and(visible, corners_img[1, :] > 0)\n",
    "    visible = np.logical_and(visible, corners_3d[2, :] > 1)\n",
    "\n",
    "    in_front = corners_3d[2, :] > 0.1  # True if a corner is at least 0.1 meter in front of the camera.\n",
    "\n",
    "    return any(visible) and all(in_front)\n",
    "\n",
    "all_pred_fn = []\n",
    "def viz_unet(sample_token,boxes): \n",
    "\n",
    "    sample = level5data.get(\"sample\", sample_token)\n",
    "\n",
    "    sample_camera_token = sample[\"data\"][\"CAM_FRONT\"]\n",
    "    camera_data = level5data.get(\"sample_data\", sample_camera_token)\n",
    "    # camera_filepath = level5data.get_sample_data_path(sample_camera_token)\n",
    "\n",
    "    ego_pose = level5data.get(\"ego_pose\", camera_data[\"ego_pose_token\"])\n",
    "    calibrated_sensor = level5data.get(\"calibrated_sensor\", camera_data[\"calibrated_sensor_token\"])\n",
    "    data_path, _, camera_intrinsic = level5data.get_sample_data(sample_camera_token)\n",
    "\n",
    "\n",
    "    data = Image.open(data_path)\n",
    "    _, axis = plt.subplots(1, 1, figsize=(9, 9))\n",
    "    \n",
    "    for i,box in enumerate(boxes):\n",
    "\n",
    "        # Move box to ego vehicle coord system\n",
    "        box.translate(-np.array(ego_pose[\"translation\"]))\n",
    "        box.rotate(Quaternion(ego_pose[\"rotation\"]).inverse)\n",
    "\n",
    "        # Move box to sensor coord system\n",
    "        box.translate(-np.array(calibrated_sensor[\"translation\"]))\n",
    "        box.rotate(Quaternion(calibrated_sensor[\"rotation\"]).inverse)\n",
    "\n",
    "        if box_in_image(box,camera_intrinsic,np.array(data).shape):            \n",
    "            box.render(axis,camera_intrinsic,normalize=True)\n",
    "\n",
    "    axis.imshow(data)\n",
    "    all_pred_fn.append(f'./cam_viz/cam_preds_{sample_token}.jpg')\n",
    "    plt.savefig(all_pred_fn[-1])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We weigh the loss for the 0 class lower to account for (some of) the big class imbalance.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_weights = torch.from_numpy(np.array([0.2] + [1.0]*len(classes), dtype=np.float32))\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading trained Unet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "batch_size = 8\n",
    "epochs = 15 # Note: We may be able to train for longer and expect better results, the reason this number is low is to keep the runtime short.\n",
    "\n",
    "model = get_unet_model(num_output_classes=len(classes)+1)\n",
    "\n",
    "state = torch.load('../input/lyft3d-mask-test-data/unet_checkpoint_epoch_10.pth', map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(state)\n",
    "model = model.to(device)\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_detection_box(prediction_opened,class_probability):\n",
    "\n",
    "    sample_boxes = []\n",
    "    sample_detection_scores = []\n",
    "    sample_detection_classes = []\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(prediction_opened, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "    \n",
    "    for cnt in contours:\n",
    "        rect = cv2.minAreaRect(cnt)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        \n",
    "        # Let's take the center pixel value as the confidence value\n",
    "        box_center_index = np.int0(np.mean(box, axis=0))\n",
    "        \n",
    "        for class_index in range(len(classes)):\n",
    "            box_center_value = class_probability[class_index+1, box_center_index[1], box_center_index[0]]\n",
    "            \n",
    "            # Let's remove candidates with very low probability\n",
    "            if box_center_value < 0.01:\n",
    "                continue\n",
    "            \n",
    "            box_center_class = classes[class_index]\n",
    "\n",
    "            box_detection_score = box_center_value\n",
    "            sample_detection_classes.append(box_center_class)\n",
    "            sample_detection_scores.append(box_detection_score)\n",
    "            sample_boxes.append(box)\n",
    "            \n",
    "    return np.array(sample_boxes),sample_detection_scores,sample_detection_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We perform an opening morphological operation to filter tiny detections\n",
    "# Note that this may be problematic for classes that are inherently small (e.g. pedestrians)..\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "    \n",
    "def open_preds(predictions_non_class0):\n",
    "\n",
    "    predictions_opened = np.zeros((predictions_non_class0.shape), dtype=np.uint8)\n",
    "\n",
    "    for i, p in enumerate(tqdm(predictions_non_class0)):\n",
    "        thresholded_p = (p > background_threshold).astype(np.uint8)\n",
    "        predictions_opened[i] = cv2.morphologyEx(thresholded_p, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "    return predictions_opened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1f5223080d48ceb6647f846da69b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3434), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2479: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False, num_workers=os.cpu_count()*2)\n",
    "progress_bar = tqdm_notebook(test_loader)\n",
    "\n",
    "# We quantize to uint8 here to conserve memory. We're allocating >20GB of memory otherwise.\n",
    "# predictions = np.zeros((len(test_loader), 1+len(classes), 336, 336), dtype=np.uint8)\n",
    "\n",
    "sample_tokens = []\n",
    "all_losses = []\n",
    "\n",
    "detection_boxes = []\n",
    "detection_scores = []\n",
    "detection_classes = []\n",
    "\n",
    "# Arbitrary threshold in our system to create a binary image to fit boxes around.\n",
    "background_threshold = 225\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for ii, (X, batch_sample_tokens) in enumerate(progress_bar):\n",
    "\n",
    "        sample_tokens.extend(batch_sample_tokens)\n",
    "        \n",
    "        X = X.to(device)  # [N, 1, H, W]\n",
    "        prediction = model(X)  # [N, 2, H, W]\n",
    "        \n",
    "        prediction = F.softmax(prediction, dim=1)\n",
    "        \n",
    "        prediction_cpu = prediction.cpu().numpy()\n",
    "        predictions = np.round(prediction_cpu*255).astype(np.uint8)\n",
    "        \n",
    "        # Get probabilities for non-background\n",
    "        predictions_non_class0 = 255 - predictions[:,0]\n",
    "        \n",
    "        predictions_opened = np.zeros((predictions_non_class0.shape), dtype=np.uint8)\n",
    "\n",
    "        for i, p in enumerate(predictions_non_class0):\n",
    "            thresholded_p = (p > background_threshold).astype(np.uint8)\n",
    "            predictions_opened[i] = cv2.morphologyEx(thresholded_p, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "            sample_boxes,sample_detection_scores,sample_detection_classes = calc_detection_box(predictions_opened[i],\n",
    "                                                                                              predictions[i])\n",
    "        \n",
    "            detection_boxes.append(np.array(sample_boxes))\n",
    "            detection_scores.append(sample_detection_scores)\n",
    "            detection_classes.append(sample_detection_classes)\n",
    "        \n",
    "#         # Visualize the first prediction\n",
    "#         if ii == 0:\n",
    "#             visualize_predictions(X, prediction, apply_softmaxiii=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of boxes: 684746\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total amount of boxes:\", np.sum([len(x) for x in detection_boxes]))\n",
    "    \n",
    "\n",
    "# Visualize the boxes in the first sample\n",
    "t = np.zeros_like(predictions_opened[0])\n",
    "for sample_boxes in detection_boxes[0]:\n",
    "    box_pix = np.int0(sample_boxes)\n",
    "    cv2.drawContours(t,[box_pix],0,(255),2)\n",
    "plt.imshow(t)\n",
    "plt.show()\n",
    "\n",
    "# Visualize their probabilities\n",
    "plt.hist(detection_scores[0], bins=20)\n",
    "plt.xlabel(\"Detection Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform predicted boxes back into world space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformation_matrix_to_voxel_space(shape, voxel_size, offset):\n",
    "    \"\"\"\n",
    "    Constructs a transformation matrix given an output voxel shape such that (0,0,0) ends up in the center.\n",
    "    Voxel_size defines how large every voxel is in world coordinate, (1,1,1) would be the same as Minecraft voxels.\n",
    "    \n",
    "    An offset per axis in world coordinates (metric) can be provided, this is useful for Z (up-down) in lidar points.\n",
    "    \"\"\"\n",
    "    \n",
    "    shape, voxel_size, offset = np.array(shape), np.array(voxel_size), np.array(offset)\n",
    "    \n",
    "    tm = np.eye(4, dtype=np.float32)\n",
    "    translation = shape/2 + offset/voxel_size\n",
    "    \n",
    "    tm = tm * np.array(np.hstack((1/voxel_size, [1])))\n",
    "    tm[:3, 3] = np.transpose(translation)\n",
    "    return tm\n",
    "\n",
    "def transform_points(points, transf_matrix):\n",
    "    \"\"\"\n",
    "    Transform (3,N) or (4,N) points using transformation matrix.\n",
    "    \"\"\"\n",
    "    if points.shape[0] not in [3,4]:\n",
    "        raise Exception(\"Points input should be (3,N) or (4,N) shape, received {}\".format(points.shape))\n",
    "    return transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]\n",
    "\n",
    "\n",
    "def car_to_voxel_coords(points, shape, voxel_size, z_offset=0):\n",
    "    if len(shape) != 3:\n",
    "        raise Exception(\"Voxel volume shape should be 3 dimensions (x,y,z)\")\n",
    "        \n",
    "    if len(points.shape) != 2 or points.shape[0] not in [3, 4]:\n",
    "        raise Exception(\"Input points should be (3,N) or (4,N) in shape, found {}\".format(points.shape))\n",
    "\n",
    "    tm = create_transformation_matrix_to_voxel_space(shape, voxel_size, (0, 0, z_offset))\n",
    "    p = transform_points(points, tm)\n",
    "    return p\n",
    "\n",
    "def create_voxel_pointcloud(points, shape, voxel_size=(0.5,0.5,1), z_offset=0):\n",
    "\n",
    "    points_voxel_coords = car_to_voxel_coords(points.copy(), shape, voxel_size, z_offset)\n",
    "    points_voxel_coords = points_voxel_coords[:3].transpose(1,0)\n",
    "    points_voxel_coords = np.int0(points_voxel_coords)\n",
    "    \n",
    "    bev = np.zeros(shape, dtype=np.float32)\n",
    "    bev_shape = np.array(shape)\n",
    "\n",
    "    within_bounds = (np.all(points_voxel_coords >= 0, axis=1) * np.all(points_voxel_coords < bev_shape, axis=1))\n",
    "    \n",
    "    points_voxel_coords = points_voxel_coords[within_bounds]\n",
    "    coord, count = np.unique(points_voxel_coords, axis=0, return_counts=True)\n",
    "        \n",
    "    # Note X and Y are flipped:\n",
    "    bev[coord[:,1], coord[:,0], coord[:,2]] = count\n",
    "    \n",
    "    return bev\n",
    "\n",
    "def normalize_voxel_intensities(bev, max_intensity=16):\n",
    "    return (bev/max_intensity).clip(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae83a3e69b746c0a335e3c3953e8bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 0/128 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video model_preds_1.mp4.\n",
      "Moviepy - Writing video model_preds_1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready model_preds_1.mp4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('./cam_viz',exist_ok=True)\n",
    "\n",
    "from moviepy.editor import ImageSequenceClip \n",
    "from lyft_dataset_sdk.eval.detection.mAP_evaluation import Box3D, recall_precision\n",
    "import shutil\n",
    "\n",
    "pred_box3ds = []\n",
    "\n",
    "max_frames = 128\n",
    "vid_count = 0\n",
    "processed_samples = 0\n",
    "for (sample_token, sample_boxes, sample_detection_scores, sample_detection_class) in tqdm_notebook(zip(sample_tokens, detection_boxes, detection_scores, detection_classes), total=len(sample_tokens)):\n",
    "    processed_samples += 1\n",
    "    sample_boxes = sample_boxes.reshape(-1, 2) # (N, 4, 2) -> (N*4, 2)\n",
    "    sample_boxes = sample_boxes.transpose(1,0) # (N*4, 2) -> (2, N*4)\n",
    "\n",
    "    # Add Z dimension\n",
    "    sample_boxes = np.vstack((sample_boxes, np.zeros(sample_boxes.shape[1]),)) # (2, N*4) -> (3, N*4)\n",
    "\n",
    "    sample = level5data.get(\"sample\", sample_token)\n",
    "    sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "    lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "    lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "    ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "    ego_translation = np.array(ego_pose['translation'])\n",
    "\n",
    "    global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                       Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "    car_from_voxel = np.linalg.inv(create_transformation_matrix_to_voxel_space(bev_shape, voxel_size, (0, 0, z_offset)))\n",
    "\n",
    "\n",
    "    global_from_voxel = np.dot(global_from_car, car_from_voxel)\n",
    "    sample_boxes = transform_points(sample_boxes, global_from_voxel)\n",
    "\n",
    "    # We don't know at where the boxes are in the scene on the z-axis (up-down), let's assume all of them are at\n",
    "    # the same height as the ego vehicle.\n",
    "    sample_boxes[2,:] = ego_pose[\"translation\"][2]\n",
    "\n",
    "\n",
    "    # (3, N*4) -> (N, 4, 3)\n",
    "    sample_boxes = sample_boxes.transpose(1,0).reshape(-1, 4, 3)\n",
    "\n",
    "#     box_height = 1.75\n",
    "    box_height = np.array([class_heights[cls] for cls in sample_detection_class])\n",
    "\n",
    "    # Note: Each of these boxes describes the ground corners of a 3D box.\n",
    "    # To get the center of the box in 3D, we'll have to add half the height to it.\n",
    "    sample_boxes_centers = sample_boxes.mean(axis=1)\n",
    "    sample_boxes_centers[:,2] += box_height/2\n",
    "\n",
    "    # Width and height is arbitrary - we don't know what way the vehicles are pointing from our prediction segmentation\n",
    "    # It doesn't matter for evaluation, so no need to worry about that here.\n",
    "    # Note: We scaled our targets to be 0.8 the actual size, we need to adjust for that\n",
    "    sample_lengths = np.linalg.norm(sample_boxes[:,0,:] - sample_boxes[:,1,:], axis=1) * 1/box_scale\n",
    "    sample_widths = np.linalg.norm(sample_boxes[:,1,:] - sample_boxes[:,2,:], axis=1) * 1/box_scale\n",
    "    \n",
    "    sample_boxes_dimensions = np.zeros_like(sample_boxes_centers) \n",
    "    sample_boxes_dimensions[:,0] = sample_widths\n",
    "    sample_boxes_dimensions[:,1] = sample_lengths\n",
    "    sample_boxes_dimensions[:,2] = box_height\n",
    "    \n",
    "    temp = []\n",
    "    for i in range(len(sample_boxes)):\n",
    "        translation = sample_boxes_centers[i]\n",
    "        size = sample_boxes_dimensions[i]\n",
    "        class_name = sample_detection_class[i]\n",
    "        ego_distance = float(np.linalg.norm(ego_translation - translation))\n",
    "    \n",
    "        \n",
    "        # Determine the rotation of the box\n",
    "        v = (sample_boxes[i,0] - sample_boxes[i,1])\n",
    "        v /= np.linalg.norm(v)\n",
    "        r = R.from_dcm([\n",
    "            [v[0], -v[1], 0],\n",
    "            [v[1],  v[0], 0],\n",
    "            [   0,     0, 1],\n",
    "        ])\n",
    "        quat = r.as_quat()\n",
    "        # XYZW -> WXYZ order of elements\n",
    "        quat = quat[[3,0,1,2]]\n",
    "        \n",
    "        detection_score = float(sample_detection_scores[i])\n",
    "\n",
    "        \n",
    "        box3d = Box(\n",
    "            token=sample_token,\n",
    "            center=list(translation),\n",
    "            size=list(size),\n",
    "            orientation=Quaternion(quat),\n",
    "            name=class_name,\n",
    "            score=detection_score\n",
    "        )\n",
    "        \n",
    "        temp.append(box3d)\n",
    "        box3d = Box3D(\n",
    "            sample_token=sample_token,\n",
    "            translation=list(translation),\n",
    "            size=list(size),\n",
    "            rotation=list(quat),\n",
    "            name=class_name,\n",
    "            score=detection_score\n",
    "        )\n",
    "        pred_box3ds.append(box3d)\n",
    "        \n",
    "#     https://github.com/Zulko/moviepy/issues/903\n",
    "    if vid_count < 1:\n",
    "        viz_unet(sample_token,temp)\n",
    "        if processed_samples==max_frames:\n",
    "            os.makedirs('./cam_viz',exist_ok=True)\n",
    "            processed_samples = 0\n",
    "            vid_count += 1        \n",
    "            new_clip = ImageSequenceClip(all_pred_fn,fps=5)\n",
    "            all_pred_fn = []\n",
    "            new_clip.write_videofile(f\"model_preds_{vid_count}.mp4\") \n",
    "            shutil.rmtree('./cam_viz')\n",
    "            del new_clip\n",
    "            gc.collect()\n",
    "            os.makedirs('./cam_viz',exist_ok=True)\n",
    "#         os.system('rm -rf ./cam_viz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./cam_viz/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11793a62f4f545c08a90936287f80133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=684746), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sub = {}\n",
    "for i in tqdm_notebook(range(len(pred_box3ds))):\n",
    "#     yaw = -np.arctan2(pred_box3ds[i].rotation[2], pred_box3ds[i].rotation[0])\n",
    "    yaw = 2*np.arccos(pred_box3ds[i].rotation[0]);\n",
    "    pred =  str(pred_box3ds[i].score/255) + ' ' + str(pred_box3ds[i].center_x)  + ' '  + \\\n",
    "    str(pred_box3ds[i].center_y) + ' '  + str(pred_box3ds[i].center_z) + ' '  + \\\n",
    "    str(pred_box3ds[i].width) + ' ' \\\n",
    "    + str(pred_box3ds[i].length) + ' '  + str(pred_box3ds[i].height) + ' ' + str(yaw) + ' ' \\\n",
    "    + str(pred_box3ds[i].name) + ' ' \n",
    "        \n",
    "    if pred_box3ds[i].sample_token in sub.keys():     \n",
    "        sub[pred_box3ds[i].sample_token] += pred\n",
    "    else:\n",
    "        sub[pred_box3ds[i].sample_token] = pred        \n",
    "    \n",
    "sample_sub = pd.read_csv('../input/3d-object-detection-for-autonomous-vehicles/sample_submission.csv')\n",
    "for token in set(sample_sub.Id.values).difference(sub.keys()):\n",
    "#     print(token)\n",
    "    sub[token] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>57b51d24fad1c39065f80c9f769b3ff8d29e17e0cf92f5...</td>\n",
       "      <td>1.0 1747.5730403343164 1274.852777166136 -19.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6d2329e7ccd8ac6469f544145bc971b7ebd69ce44a8db7...</td>\n",
       "      <td>0.9882352941176471 1772.7332509939483 1239.912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14ca96a81a2c499c150515544b9c40e92c2feba04b63f3...</td>\n",
       "      <td>0.9882352941176471 1775.570091212064 1238.1649...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3c3ed046f64803d5a78282ffedcc6ad04825af17bb33d2...</td>\n",
       "      <td>0.9803921568627451 1777.569088712561 1236.9266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7a932d3e5d2f58b103b0cc71f24866e0674476e66fcba2...</td>\n",
       "      <td>1.0 1779.898297288824 1235.4793793216784 -19.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Id  \\\n",
       "0  57b51d24fad1c39065f80c9f769b3ff8d29e17e0cf92f5...   \n",
       "1  6d2329e7ccd8ac6469f544145bc971b7ebd69ce44a8db7...   \n",
       "2  14ca96a81a2c499c150515544b9c40e92c2feba04b63f3...   \n",
       "3  3c3ed046f64803d5a78282ffedcc6ad04825af17bb33d2...   \n",
       "4  7a932d3e5d2f58b103b0cc71f24866e0674476e66fcba2...   \n",
       "\n",
       "                                    PredictionString  \n",
       "0  1.0 1747.5730403343164 1274.852777166136 -19.3...  \n",
       "1  0.9882352941176471 1772.7332509939483 1239.912...  \n",
       "2  0.9882352941176471 1775.570091212064 1238.1649...  \n",
       "3  0.9803921568627451 1777.569088712561 1236.9266...  \n",
       "4  1.0 1779.898297288824 1235.4793793216784 -19.3...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame(list(sub.items()))\n",
    "sub.columns = sample_sub.columns\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>27463</td>\n",
       "      <td>e7a1900fa84d0c252c4c53ff76b481a6dd32961383e2ea...</td>\n",
       "      <td>1.0 2435.5545878742214 863.8822947404476 -18.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27464</td>\n",
       "      <td>671c813f53d14358bd20d648eb5113facbdeff5d098e70...</td>\n",
       "      <td>1.0 2448.9496684310207 855.6496035451187 -18.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27465</td>\n",
       "      <td>147c6cb7fad6de0bb6a565247233ceab160ea212efdd72...</td>\n",
       "      <td>0.996078431372549 2436.178955755136 863.291810...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27466</td>\n",
       "      <td>2f00624fd3b1ffcd04976f73a413f65b48a993a5514f1a...</td>\n",
       "      <td>0.984313725490196 2463.549754377695 846.999500...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27467</td>\n",
       "      <td>db7eee8f5a0fd8aec6dc62530c3be5b5920cd7fa01c840...</td>\n",
       "      <td>1.0 2463.4597353499985 846.6454632893824 -18.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Id  \\\n",
       "27463  e7a1900fa84d0c252c4c53ff76b481a6dd32961383e2ea...   \n",
       "27464  671c813f53d14358bd20d648eb5113facbdeff5d098e70...   \n",
       "27465  147c6cb7fad6de0bb6a565247233ceab160ea212efdd72...   \n",
       "27466  2f00624fd3b1ffcd04976f73a413f65b48a993a5514f1a...   \n",
       "27467  db7eee8f5a0fd8aec6dc62530c3be5b5920cd7fa01c840...   \n",
       "\n",
       "                                        PredictionString  \n",
       "27463  1.0 2435.5545878742214 863.8822947404476 -18.0...  \n",
       "27464  1.0 2448.9496684310207 855.6496035451187 -18.0...  \n",
       "27465  0.996078431372549 2436.178955755136 863.291810...  \n",
       "27466  0.984313725490196 2463.549754377695 846.999500...  \n",
       "27467  1.0 2463.4597353499985 846.6454632893824 -18.8...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('lyft3d_pred.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "031f1b5a9bf141698df32a3dcd7ea15b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "0518b492900f47ee87a74a746c9a4314": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c68db17799a5461b9b716af2c3a95dfd",
        "IPY_MODEL_5b6007bc145447d7a1a6c4c10c7a655e"
       ],
       "layout": "IPY_MODEL_80b1c1ced8c040f8b729f8d5402d43b0"
      }
     },
     "0a5cb423815240dd90861623ade1c21b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b967eaa570e4cee8902a7cb9c079db5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "11793a62f4f545c08a90936287f80133": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_194c869359984297ab70daa73701975b",
        "IPY_MODEL_5d9e3e231ee94055874d55ac706ad52e"
       ],
       "layout": "IPY_MODEL_dedb2861f4ce42028d0ccbdf6b4d4cc4"
      }
     },
     "1435c74890914474b3b493bcc82e5be8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "194c869359984297ab70daa73701975b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e0f10cfac8854c67827ecfcd7d91fe51",
       "max": 684746,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_35ba4aa83efe4564b7c59f9b5d9bf033",
       "value": 684746
      }
     },
     "1f193a408f304f09b24535a65aa739ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "220ec4a6cf9c4518a9dc8c856d5da1a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "35ba4aa83efe4564b7c59f9b5d9bf033": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "3ae83a3e69b746c0a335e3c3953e8bcf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_50f0c7e0e4b54b9aa210a9ccac9b8cf4",
        "IPY_MODEL_b83c2d588f374e058ce1212d6ced9856"
       ],
       "layout": "IPY_MODEL_bd881e1aa9944327a20923190ee23662"
      }
     },
     "41debc682ac44e0d81c63908270c0f82": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "41ecb6c118244ef297e959f5a0fc40d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "50f0c7e0e4b54b9aa210a9ccac9b8cf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0a5cb423815240dd90861623ade1c21b",
       "max": 27468,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_031f1b5a9bf141698df32a3dcd7ea15b",
       "value": 27468
      }
     },
     "5b6007bc145447d7a1a6c4c10c7a655e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6c40356fd3924e079e11714e2547cbd0",
       "placeholder": "​",
       "style": "IPY_MODEL_c2aa2129cd6540ac82ef5f37c63dfe51",
       "value": " 218/218 [00:00&lt;00:00, 1537.41it/s]"
      }
     },
     "5d9e3e231ee94055874d55ac706ad52e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_41ecb6c118244ef297e959f5a0fc40d0",
       "placeholder": "​",
       "style": "IPY_MODEL_7634c68eb93948f491197d02c80d0d0f",
       "value": " 684746/684746 [00:11&lt;00:00, 61755.56it/s]"
      }
     },
     "6c40356fd3924e079e11714e2547cbd0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7634c68eb93948f491197d02c80d0d0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "80b1c1ced8c040f8b729f8d5402d43b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8630bb2c366c4b8f97b32058f363c16e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "87d65b7e7fa543509be57b4398c526e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "97701b96b8db443281f3c9608b0cbbfb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9b1f5223080d48ceb6647f846da69b4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e6e3edfb46464197a341a3ed22c6ddb1",
        "IPY_MODEL_cba95b6ee5b14019b8965ab16794a351"
       ],
       "layout": "IPY_MODEL_97701b96b8db443281f3c9608b0cbbfb"
      }
     },
     "b83c2d588f374e058ce1212d6ced9856": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_41debc682ac44e0d81c63908270c0f82",
       "placeholder": "​",
       "style": "IPY_MODEL_0b967eaa570e4cee8902a7cb9c079db5",
       "value": " 27468/27468 [07:41&lt;00:00, 59.55it/s]"
      }
     },
     "bd881e1aa9944327a20923190ee23662": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2aa2129cd6540ac82ef5f37c63dfe51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c68db17799a5461b9b716af2c3a95dfd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1f193a408f304f09b24535a65aa739ad",
       "max": 218,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_87d65b7e7fa543509be57b4398c526e2",
       "value": 218
      }
     },
     "cba95b6ee5b14019b8965ab16794a351": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8630bb2c366c4b8f97b32058f363c16e",
       "placeholder": "​",
       "style": "IPY_MODEL_220ec4a6cf9c4518a9dc8c856d5da1a8",
       "value": " 3434/3434 [08:12&lt;00:00,  6.97it/s]"
      }
     },
     "dedb2861f4ce42028d0ccbdf6b4d4cc4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e0f10cfac8854c67827ecfcd7d91fe51": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6e3edfb46464197a341a3ed22c6ddb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1435c74890914474b3b493bcc82e5be8",
       "max": 3434,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fe831a75f66748e3b468a06220f1739d",
       "value": 3434
      }
     },
     "fe831a75f66748e3b468a06220f1739d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
