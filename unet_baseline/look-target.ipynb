{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces\n",
    "# even more threads which would lead to a lot of context switching, slowing things down a lot.\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR, _LRScheduler\n",
    "import torch.nn.utils.weight_norm as weightNorm\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "\n",
    "from models.model import *\n",
    "import torchvision.models as models\n",
    "\n",
    "from utils.transform import *\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from apex import amp\n",
    "from ranger import *\n",
    "\n",
    "import albumentations\n",
    "from albumentations import torch as AT\n",
    "\n",
    "\n",
    "\n",
    "############################################################################## seed all\n",
    "SEED = 42\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "\n",
    "############################################################################## define trainsformation\n",
    "SIZE = 336\n",
    "\n",
    "\n",
    "def transform_train(image, mask):\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.RandomRotate90(p=1)(image=image)['image']\n",
    "        mask = albumentations.RandomRotate90(p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.Transpose(p=1)(image=image)['image']\n",
    "        mask = albumentations.Transpose(p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.VerticalFlip(p=1)(image=image)['image']\n",
    "        mask = albumentations.VerticalFlip(p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.HorizontalFlip(p=1)(image=image)['image']\n",
    "        mask = albumentations.HorizontalFlip(p=1)(image=mask)['image']\n",
    "\n",
    "    # if random.random() < 0.5:\n",
    "    #     image = albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.15, rotate_limit=45, p=1)(image=image)['image']\n",
    "    #     mask = albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.15, rotate_limit=45, p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.RandomBrightness(0.1)(image=image)['image']\n",
    "        image = albumentations.RandomContrast(0.1)(image=image)['image']\n",
    "        image = albumentations.Blur(blur_limit=3)(image=image)['image']\n",
    "\n",
    "    # if random.random() < 0.5:\n",
    "    #     image = albumentations.Cutout(num_holes=1, max_h_size=32, max_w_size=32, p=1)(image)\n",
    "    #     mask = albumentations.Cutout(num_holes=1, max_h_size=32, max_w_size=32, p=1)(mask)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def transform_valid(image, mask):\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.RandomRotate90(p=1)(image=image)['image']\n",
    "        mask = albumentations.RandomRotate90(p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.Transpose(p=1)(image=image)['image']\n",
    "        mask = albumentations.Transpose(p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.VerticalFlip(p=1)(image=image)['image']\n",
    "        mask = albumentations.VerticalFlip(p=1)(image=mask)['image']\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image = albumentations.HorizontalFlip(p=1)(image=image)['image']\n",
    "        mask = albumentations.HorizontalFlip(p=1)(image=mask)['image']\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def transform_test(image):\n",
    "    \n",
    "    image_hard = image.copy()\n",
    "    image_simple = image.copy()\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        image_hard = albumentations.RandomBrightness(0.1)(image=image_hard)['image']\n",
    "        image_hard = albumentations.RandomContrast(0.1)(image=image_hard)['image']\n",
    "        image_hard = albumentations.Blur(blur_limit=3)(image=image_hard)['image']\n",
    "\n",
    "    return image_simple, image_hard\n",
    "\n",
    "\n",
    "\n",
    "############################################################################## define bev dataset\n",
    "class BEVImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_filepaths=None, target_filepaths=None, type=\"train\", img_size=336, map_filepaths=None):\n",
    "        self.input_filepaths = input_filepaths\n",
    "        self.target_filepaths = target_filepaths\n",
    "        self.type = type\n",
    "        self.map_filepaths = map_filepaths\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        if map_filepaths is not None:\n",
    "            assert len(input_filepaths) == len(map_filepaths)\n",
    "        \n",
    "        assert len(input_filepaths) == len(target_filepaths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_filepath = self.input_filepaths[idx]\n",
    "        \n",
    "        sample_token = input_filepath.split(\"/\")[-1].replace(\"_input.png\",\"\")\n",
    "        \n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        if self.map_filepaths:\n",
    "            map_filepath = self.map_filepaths[idx]\n",
    "            map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "            im = np.concatenate((im, map_im), axis=2)\n",
    "\n",
    "        if (self.target_filepaths):\n",
    "            target_filepath = self.target_filepaths[idx]\n",
    "            target = cv2.imread(target_filepath, cv2.IMREAD_UNCHANGED)\n",
    "            target = target.astype(np.int64)\n",
    "        else:\n",
    "            target = None\n",
    "\n",
    "        if (self.type == \"train\"):\n",
    "            im, target = transform_train(im, target)\n",
    "        elif (self.type == \"valid\"):\n",
    "            im. target = transform_valid(im, target)\n",
    "        else:\n",
    "            im = transform_test(im)\n",
    "        \n",
    "        im = torch.from_numpy(im.transpose(2,0,1))\n",
    "        target = torch.from_numpy(target)\n",
    "        \n",
    "        return im, target, sample_token\n",
    "\n",
    "\n",
    "\n",
    "############################################################################## train test splitting 0.8 / 0.2\n",
    "train_data_folder = \"/media/jionie/my_disk/Kaggle/Lyft/input/3d-object-detection-for-autonomous-vehicles/bev_train_data/\"\n",
    "\n",
    "input_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_input.png\")))\n",
    "target_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_target.png\")))\n",
    "\n",
    "train_input_filepaths = input_filepaths[:int(0.8*len(input_filepaths))]\n",
    "train_target_filepaths = target_filepaths[:int(0.8*len(target_filepaths))]\n",
    "valid_input_filepaths = input_filepaths[int(0.8*len(input_filepaths)):]\n",
    "valid_target_filepaths = target_filepaths[int(0.8*len(target_filepaths)):]\n",
    "\n",
    "train_dataset = BEVImageDataset(input_filepaths=train_input_filepaths, target_filepaths=train_target_filepaths, type=\"train\", img_size=336, map_filepaths=None)\n",
    "valid_dataset = BEVImageDataset(input_filepaths=valid_input_filepaths, target_filepaths=valid_target_filepaths, type=\"valid\", img_size=336, map_filepaths=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, target, _ = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([336, 336])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3env",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
